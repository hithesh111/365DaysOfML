# 365DaysOfML
A commitment to learn ML (and related topics) every day for 365 days starting Jan 1 2023.

## References
1. The Elements of Statistical Learning (ESLR)
2. Serrano.Academy Youtube Channel
3. Ritvik Math Youtube Channel

#### Day 1: ESLR 2.1 to 2.3 

Introduction to Supervised Learning, Variable Types, Encodings, Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors, Other Models as a variant of these two approaches

#### Day 2: ESLR 2.4

Statistical Decision Theory: Probabilistic Setup, Conditional Mean/Median as Regression Function for Squared Loss/Absolute Loss, Linear Model Estimates and Nearest Neighbor Model Estimates from the Regression Function, Solution for Categorical Target Variable, Bayes Classifier.

#### Day 3: ESLR 2.5

Local Methods in High Dimensions: Curse of dimensionality, Nearest Neighbours not really "Near" in Nearest Neighbor models in High Dimensions, Bias-Variance Decomposition of MSE, Linear Assumption (and other Rigid Assumptions) to avoid Curse of Dimensionality.

#### Day 4: ESLR 2.6 to 2.7

Statistical Model for Pr(X,Y), Additive error Model, Supervised Learning, Function Approximation by Least Squares Method and Maximum Likelihood Method, Structured Regression Models: Using implicit or explicit neighborhood restrictions (usually complexity constraints)

#### Day 5: ESLR 2.8

Roughness Penalty or Regularization, Kernel Functions and Local Regression, Basis Functions, Splines, Dictionary Methods (Adaptively Chosen Basis Functions, eg: Neural Networks)

#### Day 6: ESLR 2.9

Model Selection and Bias - Variance Tradeoff, K Nearest Neighbours Example, Test Error, Overfitting and Underfitting

#### Day 7: Serrano.Academy Unsupervised Learning

Gaussian Mixture Models, Iterative Approach to fit a Mixture of Gaussians for Clustering.

#### Day 8: ESLR 3.1, 3.2

Linear Methods of Regression: Introduction, Generalisation and Basis Expansions, Least Square Method of finding Model Coefficients, Normality Assumptions, Significance of Coefficients

#### Day 9: ESLR 3.2.1 to 3.2.4

Significance of Linear Coefficients, Gauss Markov Theorem, Multiple Regression from Univariate Regression, Gram Schmidt Orthogonalisation to find Coefficients, Linear Regression with Multiple Outputs

#### Day 10: ESLR 3.2.1 to 3.2.4

Filling gaps from Day 9

#### Day 11: ESLR 3.3

Subset Selection: Best Subset Selection, Forward and Backward Stepwise Selection, Forward Stagewise Regression

#### Day 12: ESLR 3.4.1 and 3.4.2

Shrinkage Methods: Ridge and Lasso Regression

#### Day 13: ESLR 3.4.3

Comparison of Subset Selection, Ridge Regression and Lasso Regression

#### Day 14: ESLR 3.4.4

Least Angle Regression

#### Day 15 - ESLR 3.4.4

Least Angle Regression

#### Day 16 - ESLR 3.5.1

Methods Using Derived Input Directions: Principal Components Regression

#### Day 17 - https://atmos.washington.edu/~dennis/MatrixCalculus.pdf

Matrix Differentiation Propositions and Proofs.

#### Day 18 - ESLR 3.5.2

Partial Least Squares

#### Day 19 - ESLR 3.6

Comparison of Selection and Shrinkage Methods

#### Day 20 - ESLR 3.7

Multiple Outcome Shrinkage and Selection

#### Day 21 - ESLR 3.7

Multiple Outcome Shrinkage and Selection

#### Day 22 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Learnt about Ranking Models as opposed to Supervised and Unsupervised Models and started off the competition with a naive baseline model submission.

#### Day 23 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Added more logic to predicting the next 'cart' and 'order' item and improved the score.

#### Day 24 - Kaggle Hackathon

OTTO Multi Objective Recommender System: 

#### Day 25 - Serrano.Academy Youtube

How does Netflix recommend movies?

#### Day 26 - Serrano.Academy

KMeans and Heirarchical Clustering

#### Day 27 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Tried Label Propagation method to identify clusters structure to products from browsing order of products.

#### Day 28 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Tried one rule based method of finding key candidates (inferring from the training data) and another method of framing it as a ML problem by creating training data and ranking the probabilities

#### Day 29 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Bug Fixes and Time Optimsiation of the code.

GoDaddy Microbusiness Density Forecasting: Registered for the competition

#### Day 30 - Serrano.Academy

Dirichlet Allocation and Gibbs Sampling

#### Day 31 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Competition Deadline,Final tries

#### Day 32 - Serrano.Academy

Restricted Boltzmann Machines (RBM)

#### Day 33 - Kaggle Hackathon
 
Going through top ranked submissions.

#### Day 34 - ESLR 3.8

Incremental Foreward Stagewise Regression

#### Day 35 - ESLR 3.8

Coding Incremental Foreward Stagewise Regression from Scratch

#### Day 36 - ESLR 3.8

Piecewise Linear Path Algorithms

#### Day 37 - ESLR 3.8

Dantzig Selector

#### Day 38 - ESLR 3.8

Grouped Lasso

#### Day 39 - ESLR 3.8

Further Properties of Lasso

#### Day 40 - ESLR 3.8

Pathwise Coordinate Optimization

#### Day 41 - ESLR 3.9

Computational Considerations 

#### Day 42 - Serrano.Academy

Denoting and Variational Autoencoders

#### Day 43 - Serrano.Academy

Principal Component Analysis

#### Day 44 - ESLR 4.1

Linear Methods for Classification : Introduction

#### Day 45 - ESLR 4.2

Linear Regression of an Indicator Matrix

#### Day 46 - ESLR 4.3

Linear Discriminant Analysis

#### Day 47 - ESLR 4.3.1

Linear Discriminant Analysis: Regularised Discriminant Analysis

#### Day 48 - ESLR 4.3.3

Linear Discriminant Analysis: Reduced Rank Discriminant 

#### Day 49 - ESLR 4.3.2

Linear Discriminant Analysis: Computations for LDA

#### Day 50 - ESLR 4.4

Logistic Regression

#### Day 51 - ESLR 4.4.1

Fitting Logistic Regression Models

#### Day 52 - ESLR 4.4.2

Logistic Regression: South African Heart Disease Example

#### Day 53 - ESLR 4.4.3

Logistic Regression: Quadratic Approximations and Inference

#### Day 54 - Linkedin Learning

Completed the course Transformers: Text Classification for NLP using BERT. 
