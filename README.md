# 365DaysOfML
A commitment to learn ML (and related topics) every day for 365 days starting Jan 1 2023.

## References
1. The Elements of Statistical Learning (ESLR)
2. Serrano.Academy Youtube Channel
3. Ritvik Math Youtube Channel

#### Day 1: ESLR 2.1 to 2.3 

Introduction to Supervised Learning, Variable Types, Encodings, Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors, Other Models as a variant of these two approaches

#### Day 2: ESLR 2.4

Statistical Decision Theory: Probabilistic Setup, Conditional Mean/Median as Regression Function for Squared Loss/Absolute Loss, Linear Model Estimates and Nearest Neighbor Model Estimates from the Regression Function, Solution for Categorical Target Variable, Bayes Classifier.

#### Day 3: ESLR 2.5

Local Methods in High Dimensions: Curse of dimensionality, Nearest Neighbours not really "Near" in Nearest Neighbor models in High Dimensions, Bias-Variance Decomposition of MSE, Linear Assumption (and other Rigid Assumptions) to avoid Curse of Dimensionality.

### Day 4: ESLR 2.6 to 2.7

Statistical Model for Pr(X,Y), Additive error Model, Supervised Learning, Function Approximation by Least Squares Method and Maximum Likelihood Method, Structured Regression Models: Using implicit or explicit neighborhood restrictions (usually complexity constraints)

### Day 5: ESLR 2.8

Roughness Penalty or Regularization, Kernel Functions and Local Regression, Basis Functions, Splines, Dictionary Methods (Adaptively Chosen Basis Functions, eg: Neural Networks)

### Day 6: ESLR 2.9

Model Selection and Bias - Variance Tradeoff, K Nearest Neighbours Example, Test Error, Overfitting and Underfitting

### Day 7: Serrano.Academy Unsupervised Learning

Gaussian Mixture Models, Iterative Approach to fit a Mixture of Gaussians for Clustering.

### Day 8: ESLR 3.1, 3.2

Linear Methods of Regression: Introduction, Generalisation and Basis Expansions, Least Square Method of finding Model Coefficients, Normality Assumptions, Significance of Coefficients

### Day 9: ESLR 3.2.1 to 3.2.4

Significance of Linear Coefficients, Gauss Markov Theorem, Multiple Regression from Univariate Regression, Gram Schmidt Orthogonalisation to find Coefficients, Linear Regression with Multiple Outputs

### Day 10: ESLR 3.2.1 to 3.2.4

Filling gaps from Day 9

### Day 11: ESLR 3.3

Subset Selection: Best Subset Selection, Forward and Backward Stepwise Selection, Forward Stagewise Regression

### Day 12: ESLR 3.4.1 and 3.4.2

Shrinkage Methods: Ridge and Lasso Regression

### Day 13: ESLR 3.4.3

Comparison of Subset Selection, Ridge Regression and Lasso Regression

### Day 14: ESLR 3.4.4

Least Angle Regression

### Day 15 - ESLR 3.4.4

Least Angle Regression

### Day 16 - ESLR 3.5.1

Methods Using Derived Input Directions: Principal Components Regression

### Day 17 - https://atmos.washington.edu/~dennis/MatrixCalculus.pdf

Matrix Differentiation Propositions and Proofs.

### Day 18 - ESLR 3.5.2

Partial Least Squares

### Day 19 - ESLR 3.6

Comparison of Selection and Shrinkage Methods

### Day 20 - ESLR 3.7

Multiple Outcome Shrinkage and Selection

### Day 21 - ESLR 3.7

Multiple Outcome Shrinkage and Selection

### Day 22 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Learnt about Ranking Models as opposed to Supervised and Unsupervised Models and started off the competition with a naive baseline model submission.

### Day 23 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Added more logic to predicting the next 'cart' and 'order' item and improved the score.

### Day 24 - Kaggle Hackathon

OTTO Multi Objective Recommender System: 

### Day 25 - Serrano.Academy Youtube

How does Netflix recommend movies?

### Day 26 - Serrano.Academy

KMeans and Heirarchical Clustering

### Day 27 - Kaggle Hackathon

OTTO Multi Objective Recommender System: Tried Label Propagation method to identify clusters structure to products from browsing order of products.
